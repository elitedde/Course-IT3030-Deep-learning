import os
from keras.layers import BatchNormalization
from keras.models import Model
from keras.layers import Dense, Lambda, Input
from keras.losses import binary_crossentropy
from keras import backend as K
from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

class autoencoder:

    def __init__(self, force_learn, batch_size, mode, dim_input, num_channels, latent_size, variational=False, binary=False):
        self.latent_dim = latent_size
        self.mode = mode #example "color_float_complete"
        self.ae = None
        self.encoder = None
        self.decoder = None
        self.batch_size = batch_size
        self.original_dim = dim_input * dim_input * num_channels
        self.epsilon_std = 1.0
        self.model = 'VAE' if variational else 'AE'
        self.force_learn = force_learn
        self.create_autoencoder(variational, binary)


    def create_autoencoder(self, variational, binary):
        """
        Encoder = learns how to compress the original input into a small encoding, it finds the smallest possible representation of data
        extracting the most prominent features of the original data, it tries to find a pattern in the image
        Decoder = Learns how to restore the original data from that encoding generated by the encoder, it learns to read these compressed
        code representations and generate images based on that info

        """
        inputs = Input(batch_shape=(self.batch_size, self.original_dim))
        x1 = BatchNormalization()(inputs)
        h = Dense(512, activation='tanh')(x1)
        # h = BatchNormalization()(h)
        h = Dense(256, activation='tanh')(h)
        # h = BatchNormalization()(h)
        h = Dense(128, activation='tanh')(h)
        # h = BatchNormalization()(h)
        h = Dense(64, activation='tanh')(h)
        b1 = BatchNormalization()(h)
        if variational:
            #two latent vectors
            z_mean = Dense(self.latent_dim)(b1)
            z_log_var = Dense(self.latent_dim)(b1)
            z_mean1 = BatchNormalization()(z_mean)
            z_log_var1 = BatchNormalization()(z_log_var)
            def sampling(args):
                #the reparameterization trick with VAE is that we cannot backpropagate through a stochastic node because
                # I have a random noise
                z_mean1, z_log_var1 = args
                epsilon = K.random_normal(shape=(self.batch_size, self.latent_dim), mean=0.,
                                          stddev=self.epsilon_std)
                return z_mean1 + K.exp(z_log_var1 / 2) * epsilon
            z = Lambda(sampling, output_shape=(self.latent_dim,))([z_mean1, z_log_var1])
            self.encoder = Model(inputs, z_mean)
        else:
            z = Dense(self.latent_dim)(b1)
            self.encoder = Model(inputs, z)

        decoder_h = Dense(64, activation='tanh')
        decoder_h1 = Dense(128, activation='tanh')
        decoder_h2 = Dense(256, activation='tanh')
        decoder_h3 = Dense(512, activation='tanh')
        decoder_mean = Dense(self.original_dim, activation='sigmoid')
        h_decoded = decoder_h(z)
        h_decoded = decoder_h1(h_decoded)
        h_decoded = decoder_h2(h_decoded)
        h_decoded = decoder_h3(h_decoded)
        x_decoded_mean = decoder_mean(h_decoded)
        # auto encoder
        self.ae = Model(inputs, x_decoded_mean)
        def vae_loss(x, x_decoded_mean):
            #EBO loss function: reconstruction loss (EM) and latent loss (KL)
            #latent loss: we ensure that all the learned pools are inside the same region
            reconstruction_loss = self.original_dim * binary_crossentropy(x, x_decoded_mean)
            kl_loss = - 0.5 * K.sum(1 + z_log_var1 - K.square(z_mean1) - K.exp(z_log_var1), axis=-1)
            return K.mean(reconstruction_loss  + kl_loss)
        if variational:
            self.ae.compile(optimizer='rmsprop', loss=vae_loss, experimental_run_tf_function=False)
        elif binary:
            self.ae.compile(optimizer= 'rmsprop', loss ='binary_crossentropy', metrics=['accuracy'])
        else:
            self.ae.compile(optimizer= 'rmsprop', loss = 'mse', metrics=['accuracy'])
        self.ae.summary()
        # decoder
        decoder_input = Input(shape=(self.latent_dim,))
        _h_decoded = decoder_h(decoder_input)
        _h_decoded = decoder_h1(_h_decoded)
        _h_decoded = decoder_h2(_h_decoded)
        _h_decoded = decoder_h3(_h_decoded)
        _x_decoded_mean = decoder_mean(_h_decoded)
        self.decoder = Model(decoder_input, _x_decoded_mean)

    def train(self, gen, epochs=10):
        weights_file = './ae/'+ self.mode + "_" + self.model + "_" + str(self.latent_dim) + '.h5'
        weights_file2 = './ae/ ' + self.mode + "_" + self.model + "_" + str(self.latent_dim) + '.h5'
        if os.path.exists(weights_file)  and self.force_learn is not True:
            self.ae.load_weights(weights_file)
            print('Loaded weights!')
        elif os.path.exists(weights_file2) and self.force_learn is not True:
            self.ae.load_weights(weights_file2)
            print('Loaded weights!')
        else:
            if 'missing' in self.mode:
                x_train, _ = gen.get_random_batch(training = True, batch_size=5000)
                x_val, _ = gen.get_random_batch(training=False, batch_size=5000)
            else:
                x_train, _ = gen.get_full_data_set(training=True)
                x_val, _ = gen.get_full_data_set(training=False)

            x_train, x_val = x_train.reshape(x_train.shape[0], -1), x_val.reshape(x_val.shape[0], -1)
            self.ae.fit(x_train, x_train, epochs=epochs, batch_size= self.batch_size, verbose=2, validation_data=(x_val, x_val))
            print("-------AE has been trained-----")
            self.ae.save_weights('./ae/'+ self.mode + "_" + self.model + "_" + str(self.latent_dim) + '.h5')

    def predict(self, x):
        return self.ae.predict(x)


